{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c974a32",
   "metadata": {},
   "source": [
    "# PVAnalytics QA Process: Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fd874e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pvanalytics\n",
    "import numpy as np\n",
    "import rdtools\n",
    "from statistics import mode\n",
    "import json\n",
    "# pvanalytics.__version__\n",
    "from pvanalytics.features.clearsky import reno       #update to just do a pvanalytics import?\n",
    "import pvlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pvanalytics.quality import data_shifts as ds\n",
    "from pvanalytics.quality import gaps\n",
    "from pvanalytics.quality.outliers import zscore\n",
    "from pvanalytics.features.daytime import power_or_irradiance\n",
    "from pvanalytics.quality.time import shifts_ruptures\n",
    "from pvanalytics.features import daytime\n",
    "from pvanalytics.system import (is_tracking_envelope,\n",
    "                                infer_orientation_fit_pvwatts)\n",
    "from pvanalytics.features.clipping import geometric\n",
    "import ruptures as rpt\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 12,\n",
    "                           'figure.figsize': [4.5, 3],\n",
    "                           'lines.markeredgewidth': 0,\n",
    "                           'lines.markersize': 2\n",
    "                           })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a0181d",
   "metadata": {},
   "source": [
    "In the following example, a process for assessing the data quality of an irradiance data stream is shown, using PVAnalytics functions. This example pipeline illustrates how several PVAnalytics functions can be used in sequence to assess the quality of an irradiance data stream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5712968d",
   "metadata": {},
   "source": [
    "First, we download and import the power data from a PV installation under the [2023 solar data prize data set](https://data.openei.org/s3_viewer?bucket=oedi-data-lake&limit=100&prefix=pvdaq%2F2023-solar-data-prize%2F). This data set is publicly available via the PVDAQ database in the DOE Open Energy Data Initiative (OEDI) (https://data.openei.org/submissions/4568), under system ID 2107. This data is timezone-localized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff00ca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local CSV file not found. Downloading from S3.\n"
     ]
    },
    {
     "ename": "NoCredentialsError",
     "evalue": "Unable to locate credentials",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoCredentialsError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 29\u001b[0m\n\u001b[0;32m     22\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[0;32m     23\u001b[0m         file_path,\n\u001b[0;32m     24\u001b[0m         index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     25\u001b[0m         parse_dates\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     26\u001b[0m     )\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m---> 29\u001b[0m df_elect \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/2107_electrical_data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43ms3_bucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moedi-data-lake\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43ms3_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpvdaq/2023-solar-data-prize/2107_OEDI/data/2107_electrical_data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#df_elect = parse_prize_csv('2107_electrical_data.csv', tz)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m power_columns \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m df_elect\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpower\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m x]\n",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(filename, s3_bucket, s3_key)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLocal CSV file not found. Downloading from S3.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m     \u001b[43mdownload_csv_from_s3\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms3_bucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms3_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m data_frame \u001b[38;5;241m=\u001b[39m load_csv(local_file_path)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_frame\n",
      "Cell \u001b[1;32mIn[9], line 19\u001b[0m, in \u001b[0;36mdownload_csv_from_s3\u001b[1;34m(bucket_name, s3_key, local_destination)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_csv_from_s3\u001b[39m(bucket_name, s3_key, local_destination):\n\u001b[0;32m     18\u001b[0m     s3 \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m     \u001b[43ms3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms3_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_destination\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pvfleets_qa_analysis\\lib\\site-packages\\boto3\\s3\\inject.py:190\u001b[0m, in \u001b[0;36mdownload_file\u001b[1;34m(self, Bucket, Key, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Download an S3 object to a file.\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mUsage::\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m    transfer.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m S3Transfer(\u001b[38;5;28mself\u001b[39m, Config) \u001b[38;5;28;01mas\u001b[39;00m transfer:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransfer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBucket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mKey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mExtraArgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pvfleets_qa_analysis\\lib\\site-packages\\boto3\\s3\\transfer.py:326\u001b[0m, in \u001b[0;36mS3Transfer.download_file\u001b[1;34m(self, bucket, key, filename, extra_args, callback)\u001b[0m\n\u001b[0;32m    322\u001b[0m future \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39mdownload(\n\u001b[0;32m    323\u001b[0m     bucket, key, filename, extra_args, subscribers\n\u001b[0;32m    324\u001b[0m )\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 326\u001b[0m     \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;66;03m# This is for backwards compatibility where when retries are\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;66;03m# exceeded we need to throw the same error from boto3 instead of\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# s3transfer's built in RetriesExceededError as current users are\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# catching the boto3 one instead of the s3transfer exception to do\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# their own retries.\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m S3TransferRetriesExceededError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pvfleets_qa_analysis\\lib\\site-packages\\s3transfer\\futures.py:103\u001b[0m, in \u001b[0;36mTransferFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;66;03m# Usually the result() method blocks until the transfer is done,\u001b[39;00m\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;66;03m# however if a KeyboardInterrupt is raised we want want to exit\u001b[39;00m\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;66;03m# out of this and propagate the exception.\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_coordinator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    105\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pvfleets_qa_analysis\\lib\\site-packages\\s3transfer\\futures.py:266\u001b[0m, in \u001b[0;36mTransferCoordinator.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# Once done waiting, raise an exception if present or return the\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# final result.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m--> 266\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pvfleets_qa_analysis\\lib\\site-packages\\s3transfer\\tasks.py:269\u001b[0m, in \u001b[0;36mSubmissionTask._main\u001b[1;34m(self, transfer_future, **kwargs)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_coordinator\u001b[38;5;241m.\u001b[39mset_status_to_running()\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;66;03m# Call the submit method to start submitting tasks to execute the\u001b[39;00m\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;66;03m# transfer.\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_submit(transfer_future\u001b[38;5;241m=\u001b[39mtransfer_future, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;66;03m# If there was an exception raised during the submission of task\u001b[39;00m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m# there is a chance that the final task that signals if a transfer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m \n\u001b[0;32m    282\u001b[0m     \u001b[38;5;66;03m# Set the exception, that caused the process to fail.\u001b[39;00m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_and_set_exception(e)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pvfleets_qa_analysis\\lib\\site-packages\\s3transfer\\download.py:354\u001b[0m, in \u001b[0;36mDownloadSubmissionTask._submit\u001b[1;34m(self, client, config, osutil, request_executor, io_executor, transfer_future, bandwidth_limiter)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;124;03m:param client: The client associated with the transfer manager\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m    downloading streams\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transfer_future\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39msize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;66;03m# If a size was not provided figure out the size for the\u001b[39;00m\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;66;03m# user.\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m     response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mhead_object(\n\u001b[0;32m    355\u001b[0m         Bucket\u001b[38;5;241m=\u001b[39mtransfer_future\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mcall_args\u001b[38;5;241m.\u001b[39mbucket,\n\u001b[0;32m    356\u001b[0m         Key\u001b[38;5;241m=\u001b[39mtransfer_future\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mcall_args\u001b[38;5;241m.\u001b[39mkey,\n\u001b[0;32m    357\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtransfer_future\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mcall_args\u001b[38;5;241m.\u001b[39mextra_args,\n\u001b[0;32m    358\u001b[0m     )\n\u001b[0;32m    359\u001b[0m     transfer_future\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mprovide_transfer_size(\n\u001b[0;32m    360\u001b[0m         response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContentLength\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    361\u001b[0m     )\n\u001b[0;32m    363\u001b[0m download_output_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_download_output_manager_cls(\n\u001b[0;32m    364\u001b[0m     transfer_future, osutil\n\u001b[0;32m    365\u001b[0m )(osutil, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_coordinator, io_executor)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pvfleets_qa_analysis\\lib\\site-packages\\botocore\\client.py:535\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m     )\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pvfleets_qa_analysis\\lib\\site-packages\\botocore\\client.py:963\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m    959\u001b[0m     maybe_compress_request(\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mconfig, request_dict, operation_model\n\u001b[0;32m    961\u001b[0m     )\n\u001b[0;32m    962\u001b[0m     apply_request_checksum(request_dict)\n\u001b[1;32m--> 963\u001b[0m     http, parsed_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_context\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39memit(\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter-call.\u001b[39m\u001b[38;5;132;01m{service_id}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{operation_name}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    969\u001b[0m         service_id\u001b[38;5;241m=\u001b[39mservice_id, operation_name\u001b[38;5;241m=\u001b[39moperation_name\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    974\u001b[0m     context\u001b[38;5;241m=\u001b[39mrequest_context,\n\u001b[0;32m    975\u001b[0m )\n\u001b[0;32m    977\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pvfleets_qa_analysis\\lib\\site-packages\\botocore\\client.py:986\u001b[0m, in \u001b[0;36mBaseClient._make_request\u001b[1;34m(self, operation_model, request_dict, request_context)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_model, request_dict, request_context):\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 986\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_endpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    987\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39memit(\n\u001b[0;32m    989\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter-call-error.\u001b[39m\u001b[38;5;132;01m{service_id}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{operation_name}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    990\u001b[0m                 service_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_model\u001b[38;5;241m.\u001b[39mservice_id\u001b[38;5;241m.\u001b[39mhyphenize(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    994\u001b[0m             context\u001b[38;5;241m=\u001b[39mrequest_context,\n\u001b[0;32m    995\u001b[0m         )\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pvfleets_qa_analysis\\lib\\site-packages\\botocore\\endpoint.py:119\u001b[0m, in \u001b[0;36mEndpoint.make_request\u001b[1;34m(self, operation_model, request_dict)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_model, request_dict):\n\u001b[0;32m    114\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaking request for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m with params: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    116\u001b[0m         operation_model,\n\u001b[0;32m    117\u001b[0m         request_dict,\n\u001b[0;32m    118\u001b[0m     )\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pvfleets_qa_analysis\\lib\\site-packages\\botocore\\endpoint.py:198\u001b[0m, in \u001b[0;36mEndpoint._send_request\u001b[1;34m(self, request_dict, operation_model)\u001b[0m\n\u001b[0;32m    196\u001b[0m context \u001b[38;5;241m=\u001b[39m request_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_retries_context(context, attempts)\n\u001b[1;32m--> 198\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m success_response, exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_response(\n\u001b[0;32m    200\u001b[0m     request, operation_model, context\n\u001b[0;32m    201\u001b[0m )\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_retry(\n\u001b[0;32m    203\u001b[0m     attempts,\n\u001b[0;32m    204\u001b[0m     operation_model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    207\u001b[0m     exception,\n\u001b[0;32m    208\u001b[0m ):\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pvfleets_qa_analysis\\lib\\site-packages\\botocore\\endpoint.py:134\u001b[0m, in \u001b[0;36mEndpoint.create_request\u001b[1;34m(self, params, operation_model)\u001b[0m\n\u001b[0;32m    130\u001b[0m     service_id \u001b[38;5;241m=\u001b[39m operation_model\u001b[38;5;241m.\u001b[39mservice_model\u001b[38;5;241m.\u001b[39mservice_id\u001b[38;5;241m.\u001b[39mhyphenize()\n\u001b[0;32m    131\u001b[0m     event_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequest-created.\u001b[39m\u001b[38;5;132;01m{service_id}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{op_name}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    132\u001b[0m         service_id\u001b[38;5;241m=\u001b[39mservice_id, op_name\u001b[38;5;241m=\u001b[39moperation_model\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m    133\u001b[0m     )\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event_emitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m prepared_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_request(request)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prepared_request\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pvfleets_qa_analysis\\lib\\site-packages\\botocore\\hooks.py:412\u001b[0m, in \u001b[0;36mEventAliaser.emit\u001b[1;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21memit\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    411\u001b[0m     aliased_event_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alias_event_name(event_name)\n\u001b[1;32m--> 412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_emitter\u001b[38;5;241m.\u001b[39memit(aliased_event_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pvfleets_qa_analysis\\lib\\site-packages\\botocore\\hooks.py:256\u001b[0m, in \u001b[0;36mHierarchicalEmitter.emit\u001b[1;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21memit\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    246\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;124;03m    Emit an event by name with arguments passed as keyword args.\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;124;03m             handlers.\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_emit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pvfleets_qa_analysis\\lib\\site-packages\\botocore\\hooks.py:239\u001b[0m, in \u001b[0;36mHierarchicalEmitter._emit\u001b[1;34m(self, event_name, kwargs, stop_on_response)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers_to_call:\n\u001b[0;32m    238\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: calling handler \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, event_name, handler)\n\u001b[1;32m--> 239\u001b[0m     response \u001b[38;5;241m=\u001b[39m handler(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    240\u001b[0m     responses\u001b[38;5;241m.\u001b[39mappend((handler, response))\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_on_response \u001b[38;5;129;01mand\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pvfleets_qa_analysis\\lib\\site-packages\\botocore\\signers.py:105\u001b[0m, in \u001b[0;36mRequestSigner.handler\u001b[1;34m(self, operation_name, request, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;66;03m# This is typically hooked up to the \"request-created\" event\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;66;03m# from a client's event emitter.  When a new request is created\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;66;03m# this method is invoked to sign the request.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;66;03m# Don't call this method directly.\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msign\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pvfleets_qa_analysis\\lib\\site-packages\\botocore\\signers.py:189\u001b[0m, in \u001b[0;36mRequestSigner.sign\u001b[1;34m(self, operation_name, request, region_name, signing_type, expires_in, signing_name)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m--> 189\u001b[0m \u001b[43mauth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_auth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pvfleets_qa_analysis\\lib\\site-packages\\botocore\\auth.py:418\u001b[0m, in \u001b[0;36mSigV4Auth.add_auth\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_auth\u001b[39m(\u001b[38;5;28mself\u001b[39m, request):\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcredentials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NoCredentialsError()\n\u001b[0;32m    419\u001b[0m     datetime_now \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mutcnow()\n\u001b[0;32m    420\u001b[0m     request\u001b[38;5;241m.\u001b[39mcontext[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m datetime_now\u001b[38;5;241m.\u001b[39mstrftime(SIGV4_TIMESTAMP)\n",
      "\u001b[1;31mNoCredentialsError\u001b[0m: Unable to locate credentials"
     ]
    }
   ],
   "source": [
    "with open('./data/2107_system_metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "tz = metadata['System']['timezone_code']\n",
    "\n",
    "def load_data(filename, s3_bucket, s3_key):\n",
    "    local_file_path = filename\n",
    "    # Check if the file exists locally\n",
    "    if os.path.exists(local_file_path):\n",
    "        print(f\"Loading local CSV file: {local_file_path}\")\n",
    "    else:\n",
    "        print(f\"Local CSV file not found. Downloading from S3.\")\n",
    "        download_csv_from_s3(s3_bucket, s3_key, local_file_path)\n",
    "    data_frame = load_csv(local_file_path)\n",
    "    return data_frame\n",
    " \n",
    "def download_csv_from_s3(bucket_name, s3_key, local_destination):\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    s3.download_file(bucket_name, s3_key, local_destination)\n",
    " \n",
    "def load_csv(file_path):\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        index_col=0,\n",
    "        parse_dates=[0],\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df_elect = load_data(\n",
    "    filename=\"./data/2107_electrical_data.csv\",\n",
    "    s3_bucket=\"oedi-data-lake\",\n",
    "    s3_key=\"pvdaq/2023-solar-data-prize/2107_OEDI/data/2107_electrical_data.csv\")\n",
    "\n",
    "#df_elect = parse_prize_csv('2107_electrical_data.csv', tz)\n",
    "\n",
    "power_columns = [x for x in df_elect.columns if 'power' in x]\n",
    "\n",
    "latitude = metadata['Site']['latitude']\n",
    "longitude = metadata['Site']['longitude']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33799df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in power_columns:\n",
    "    power_time_series = df_elect[col].copy()\n",
    "\n",
    "    # Get the time frequency of the time series\n",
    "    freq_minutes = int((mode(abs(np.diff(power_time_series.index)))).total_seconds() / 60)\n",
    "    data_freq = str(freq_minutes) + \"min\"\n",
    "    power_time_series = power_time_series.asfreq(data_freq)\n",
    "\n",
    "    # REMOVE STALE DATA (that isn't during nighttime periods)\n",
    "    # Day/night mask\n",
    "    daytime_mask = power_or_irradiance(power_time_series)\n",
    "    # Stale data mask\n",
    "    stale_data_mask = gaps.stale_values_round(power_time_series,\n",
    "                                              window=3,\n",
    "                                              decimals=2)\n",
    "    stale_data_mask = stale_data_mask & daytime_mask\n",
    "\n",
    "    # REMOVE NEGATIVE DATA\n",
    "    negative_mask = (power_time_series < 0)\n",
    "\n",
    "    # FIND ABNORMAL PERIODS\n",
    "    daily_min = power_time_series.resample('D').min()\n",
    "    series_min = 0.1 * power_time_series.mean()\n",
    "    erroneous_mask = (daily_min >= series_min)\n",
    "    erroneous_mask = erroneous_mask.reindex(index=power_time_series.index,\n",
    "                                            method='ffill',\n",
    "                                            fill_value=False)\n",
    "\n",
    "    # FIND OUTLIERS (Z-SCORE FILTER)\n",
    "    zscore_outlier_mask = zscore(power_time_series, zmax=4,\n",
    "                                 nan_policy='omit')\n",
    "\n",
    "    # Get the percentage of data flagged for each issue, so it can later be logged\n",
    "    pct_stale = round((len(power_time_series[\n",
    "        stale_data_mask].dropna())/len(power_time_series.dropna())*100), 1)\n",
    "    pct_negative = round((len(power_time_series[\n",
    "        negative_mask].dropna())/len(power_time_series.dropna())*100), 1)\n",
    "    pct_erroneous = round((len(power_time_series[\n",
    "        erroneous_mask].dropna())/len(power_time_series.dropna())*100), 1)\n",
    "    pct_outlier = round((len(power_time_series[\n",
    "        zscore_outlier_mask].dropna())/len(power_time_series.dropna())*100), 1)\n",
    "\n",
    "\n",
    "\n",
    "    # Filter the time series, taking out all of the issues\n",
    "    issue_mask = ((~stale_data_mask) & (~negative_mask) &\n",
    "              (~erroneous_mask) & (~zscore_outlier_mask))\n",
    "\n",
    "    power_time_series = power_time_series[issue_mask].copy()\n",
    "    power_time_series = power_time_series.asfreq(data_freq)\n",
    "\n",
    "\n",
    "    # daily data completeness\n",
    "    x = power_time_series.copy()\n",
    "    x.loc[~daytime_mask] = 0\n",
    "    data_completeness_score = gaps.completeness_score(x)\n",
    "\n",
    "\n",
    "    # Trim the series based on daily completeness score\n",
    "    trim_series = pvanalytics.quality.gaps.trim_incomplete(\n",
    "        x, minimum_completeness=.25, freq=data_freq)\n",
    "    # first_valid_date, last_valid_date = \\\n",
    "    #     pvanalytics.quality.gaps.start_stop_dates(trim_series)\n",
    "    # if first_valid_date is not None:\n",
    "    #     power_time_series = power_time_series[first_valid_date.tz_convert(power_time_series.index.tz):\n",
    "    #                               last_valid_date.tz_convert(power_time_series.index.tz)]\n",
    "\n",
    "    power_time_series = power_time_series[trim_series].copy()\n",
    "    power_time_series = power_time_series.asfreq(data_freq)\n",
    "\n",
    "    # Get time of day from the associated datetime column\n",
    "    time_of_day = pd.Series(power_time_series.index.hour +\n",
    "                            power_time_series.index.minute/60,\n",
    "                            index=power_time_series.index)\n",
    "    # Pivot the dataframe\n",
    "    dataframe = pd.DataFrame(pd.concat([power_time_series, time_of_day], axis=1))\n",
    "    dataframe.columns = [\"values\", 'time_of_day']\n",
    "    dataframe = dataframe.dropna()\n",
    "    dataframe_pivoted = dataframe.pivot_table(index='time_of_day',\n",
    "                                              columns=dataframe.index.date,\n",
    "                                              values=\"values\")\n",
    "\n",
    "    # Get the modeled sunrise and sunset time series based on the system's\n",
    "    # latitude-longitude coordinates\n",
    "    modeled_sunrise_sunset_df = pvlib.solarposition.sun_rise_set_transit_spa(\n",
    "         power_time_series.index, latitude, longitude)\n",
    "\n",
    "    # Calculate the midday point between sunrise and sunset for each day\n",
    "    # in the modeled irradiance series\n",
    "    modeled_midday_series = modeled_sunrise_sunset_df['sunrise'] + \\\n",
    "        (modeled_sunrise_sunset_df['sunset'] -\n",
    "         modeled_sunrise_sunset_df['sunrise']) / 2\n",
    "\n",
    "    # Run day-night mask on the power time series\n",
    "    daytime_mask = power_or_irradiance(power_time_series,\n",
    "                                       freq=data_freq,\n",
    "                                       low_value_threshold=.005)\n",
    "\n",
    "    # Generate the sunrise, sunset, and halfway points for the data stream\n",
    "    sunrise_series = daytime.get_sunrise(daytime_mask)\n",
    "    sunset_series = daytime.get_sunset(daytime_mask)\n",
    "    midday_series = sunrise_series + ((sunset_series - sunrise_series)/2)\n",
    "\n",
    "    # Convert the midday and modeled midday series to daily values\n",
    "    midday_series_daily, modeled_midday_series_daily = (\n",
    "        midday_series.resample('D').mean(),\n",
    "        modeled_midday_series.resample('D').mean())\n",
    "\n",
    "    # Set midday value series as minutes since midnight, from midday datetime\n",
    "    # values\n",
    "    midday_series_daily = (midday_series_daily.dt.hour * 60 +\n",
    "                           midday_series_daily.dt.minute +\n",
    "                           midday_series_daily.dt.second / 60)\n",
    "    modeled_midday_series_daily = \\\n",
    "        (modeled_midday_series_daily.dt.hour * 60 +\n",
    "         modeled_midday_series_daily.dt.minute +\n",
    "         modeled_midday_series_daily.dt.second / 60)\n",
    "\n",
    "    # Estimate the time shifts by comparing the modelled midday point to the\n",
    "    # measured midday point.\n",
    "    is_shifted, time_shift_series = shifts_ruptures(midday_series_daily,\n",
    "                                                    modeled_midday_series_daily,\n",
    "                                                    period_min=15,\n",
    "                                                    shift_min=15,\n",
    "                                                    zscore_cutoff=1.5)\n",
    "\n",
    "    # Create a midday difference series between modeled and measured midday, to\n",
    "    # visualize time shifts. First, resample each time series to daily frequency,\n",
    "    # and compare the data stream's daily halfway point to the modeled halfway\n",
    "    # point\n",
    "    midday_diff_series = (midday_series.resample('D').mean() -\n",
    "                          modeled_midday_series.resample('D').mean()\n",
    "                          ).dt.total_seconds() / 60\n",
    "\n",
    "    # Generate boolean for detected time shifts\n",
    "    if any(time_shift_series != 0):\n",
    "        time_shifts_detected = True\n",
    "    else:\n",
    "        time_shifts_detected = False\n",
    "\n",
    "    # Build a list of time shifts for re-indexing. We choose to use dicts.\n",
    "    time_shift_series.index = pd.to_datetime(\n",
    "        time_shift_series.index)\n",
    "    changepoints = (time_shift_series != time_shift_series.shift(1))\n",
    "    changepoints = changepoints[changepoints].index\n",
    "    changepoint_amts = pd.Series(time_shift_series.loc[changepoints])\n",
    "    time_shift_list = list()\n",
    "    for idx in range(len(changepoint_amts)):\n",
    "        if changepoint_amts[idx] == 0:\n",
    "            change_amt = 0\n",
    "        else:\n",
    "            change_amt = -1 * changepoint_amts[idx]\n",
    "        if idx < (len(changepoint_amts) - 1):\n",
    "            time_shift_list.append({\"datetime_start\":\n",
    "                                    str(changepoint_amts.index[idx]),\n",
    "                                    \"datetime_end\":\n",
    "                                        str(changepoint_amts.index[idx + 1]),\n",
    "                                    \"time_shift\": change_amt})\n",
    "        else:\n",
    "            time_shift_list.append({\"datetime_start\":\n",
    "                                    str(changepoint_amts.index[idx]),\n",
    "                                    \"datetime_end\":\n",
    "                                        str(time_shift_series.index.max()),\n",
    "                                    \"time_shift\": change_amt})\n",
    "\n",
    "    # Correct any time shifts in the time series\n",
    "    new_index = pd.Series(power_time_series.index, index=power_time_series.index).dropna()\n",
    "    for i in time_shift_list:\n",
    "        if pd.notna(i['time_shift']):\n",
    "            new_index[(power_time_series.index >= pd.to_datetime(i['datetime_start'])) &\n",
    "                  (power_time_series.index < pd.to_datetime(i['datetime_end']))] = \\\n",
    "            power_time_series.index + pd.Timedelta(minutes=i['time_shift'])\n",
    "    power_time_series.index = new_index\n",
    "\n",
    "    # Remove duplicated indices and sort the time series (just in case)\n",
    "    power_time_series = power_time_series[~power_time_series.index.duplicated(\n",
    "        keep='first')].sort_index()\n",
    "\n",
    "    # Set all values in the nighttime mask to 0\n",
    "    power_time_series.loc[~daytime_mask] = 0\n",
    "    # Resample the time series to daily mean\n",
    "    power_time_series_daily = power_time_series.resample('D').mean()\n",
    "    data_shift_start_date, data_shift_end_date = \\\n",
    "        ds.get_longest_shift_segment_dates(power_time_series_daily,\n",
    "                                           use_default_models=False,\n",
    "                                           method=rpt.Binseg, cost='rbf',\n",
    "                                           penalty=15)\n",
    "    data_shift_period_length = (data_shift_end_date -\n",
    "                                data_shift_start_date).days\n",
    "\n",
    "    # Get the number of shift dates\n",
    "    data_shift_mask = ds.detect_data_shifts(power_time_series_daily,\n",
    "                                            use_default_models=False,\n",
    "                                            method=rpt.Binseg, cost='rbf',\n",
    "                                            penalty=15)\n",
    "    # Get the shift dates\n",
    "    shift_dates = list(power_time_series_daily[data_shift_mask].index)\n",
    "    if len(shift_dates) > 0:\n",
    "        shift_found = True\n",
    "    else:\n",
    "        shift_found = False\n",
    "    \n",
    "    power_time_series = power_time_series[\n",
    "        (power_time_series.index >=\n",
    "         data_shift_start_date.tz_convert(power_time_series.index.tz)) &\n",
    "        (power_time_series.index <=\n",
    "         data_shift_end_date.tz_convert(power_time_series.index.tz))]\n",
    "\n",
    "    power_time_series = power_time_series.asfreq(data_freq)\n",
    "\n",
    "    #power_time_series = power_time_series.tz_convert(\"UTC\")\n",
    "    #power_time_series.to_csv(os.path.join(\"./filtered_qa\",  col + \".csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
