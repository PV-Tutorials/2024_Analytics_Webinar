{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d7e9ae7",
   "metadata": {},
   "source": [
    "# PVAnalytics QA Process: Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f145319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pvanalytics\n",
    "import numpy as np\n",
    "import rdtools\n",
    "from statistics import mode\n",
    "import json\n",
    "# pvanalytics.__version__\n",
    "from pvanalytics.features.clearsky import reno       #update to just do a pvanalytics import?\n",
    "import pvlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pvanalytics.quality import data_shifts as ds\n",
    "from pvanalytics.quality import gaps\n",
    "from pvanalytics.quality.outliers import zscore\n",
    "from pvanalytics.features.daytime import power_or_irradiance\n",
    "from pvanalytics.quality.time import shifts_ruptures\n",
    "from pvanalytics.features import daytime\n",
    "from pvanalytics.system import (is_tracking_envelope,\n",
    "                                infer_orientation_fit_pvwatts)\n",
    "from pvanalytics.features.clipping import geometric\n",
    "import ruptures as rpt\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 12,\n",
    "                           'figure.figsize': [4.5, 3],\n",
    "                           'lines.markeredgewidth': 0,\n",
    "                           'lines.markersize': 2\n",
    "                           })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca033a1",
   "metadata": {},
   "source": [
    "In the following example, a process for assessing the data quality of the AC power data streams for system 2107 is shown, using PVAnalytics functions. This example pipeline illustrates how several PVAnalytics functions can be used in sequence to assess the quality of a power data stream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8802c0a2",
   "metadata": {},
   "source": [
    "First, we download and import the power data from a PV installation under the [2023 solar data prize data set](https://data.openei.org/s3_viewer?bucket=oedi-data-lake&limit=100&prefix=pvdaq%2F2023-solar-data-prize%2F). This data set is publicly available via the PVDAQ database in the DOE Open Energy Data Initiative (OEDI) (https://data.openei.org/submissions/4568), under system ID 2107. This data is timezone-localized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5d50393",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/2107_system_metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "tz = metadata['System']['timezone_code']\n",
    "\n",
    "def load_csv(file_path):\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        index_col=0,\n",
    "        parse_dates=True,\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df_elect = load_csv(\"./data/2107_electrical_data.csv\")\n",
    "\n",
    "df_elect = df_elect.tz_localize(tz, ambiguous=True)\n",
    "\n",
    "power_columns = [x for x in df_elect.columns if 'power' in x]\n",
    "\n",
    "latitude = metadata['Site']['latitude']\n",
    "longitude = metadata['Site']['longitude']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eb3635",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in power_columns:\n",
    "    power_time_series = df_elect[col].copy()\n",
    "\n",
    "    # Get the time frequency of the time series\n",
    "    freq_minutes = mode(power_time_series.index.to_series().diff().dt.seconds / 60)\n",
    "    data_freq = str(freq_minutes) + \"min\"\n",
    "    power_time_series = power_time_series.asfreq(data_freq)    \n",
    "\n",
    "    # REMOVE STALE DATA (that isn't during nighttime periods)\n",
    "    # Day/night mask\n",
    "    daytime_mask = power_or_irradiance(power_time_series)\n",
    "    # Stale data mask\n",
    "    stale_data_mask = gaps.stale_values_round(power_time_series,\n",
    "                                              window=3,\n",
    "                                              decimals=2)\n",
    "    stale_data_mask = stale_data_mask & daytime_mask\n",
    "\n",
    "    # REMOVE NEGATIVE DATA\n",
    "    negative_mask = (power_time_series < 0)\n",
    "\n",
    "    # FIND ABNORMAL PERIODS\n",
    "    daily_min = power_time_series.resample('D').min()\n",
    "    series_min = 0.1 * power_time_series.mean()\n",
    "    erroneous_mask = (daily_min >= series_min)\n",
    "    erroneous_mask = erroneous_mask.reindex(index=power_time_series.index,\n",
    "                                            method='ffill',\n",
    "                                            fill_value=False)\n",
    "\n",
    "    # FIND OUTLIERS (Z-SCORE FILTER)\n",
    "    zscore_outlier_mask = zscore(power_time_series, zmax=4,\n",
    "                                 nan_policy='omit')\n",
    "\n",
    "    # Get the percentage of data flagged for each issue, so it can later be logged\n",
    "    pct_stale = round((len(power_time_series[\n",
    "        stale_data_mask].dropna())/len(power_time_series.dropna())*100), 1)\n",
    "    pct_negative = round((len(power_time_series[\n",
    "        negative_mask].dropna())/len(power_time_series.dropna())*100), 1)\n",
    "    pct_erroneous = round((len(power_time_series[\n",
    "        erroneous_mask].dropna())/len(power_time_series.dropna())*100), 1)\n",
    "    pct_outlier = round((len(power_time_series[\n",
    "        zscore_outlier_mask].dropna())/len(power_time_series.dropna())*100), 1)\n",
    "\n",
    "\n",
    "\n",
    "    # Filter the time series, taking out all of the issues\n",
    "    issue_mask = ((~stale_data_mask) & (~negative_mask) &\n",
    "              (~erroneous_mask) & (~zscore_outlier_mask))\n",
    "\n",
    "    power_time_series = power_time_series[issue_mask].copy()\n",
    "    power_time_series = power_time_series.asfreq(data_freq)\n",
    "\n",
    "\n",
    "    # daily data completeness\n",
    "    x = power_time_series.copy()\n",
    "    x.loc[~daytime_mask] = 0\n",
    "    data_completeness_score = gaps.completeness_score(x)\n",
    "\n",
    "\n",
    "    # Trim the series based on daily completeness score\n",
    "    trim_series = pvanalytics.quality.gaps.trim_incomplete(\n",
    "        x, minimum_completeness=.25, freq=data_freq)\n",
    "\n",
    "    power_time_series = power_time_series[trim_series].copy()\n",
    "    power_time_series = power_time_series.asfreq(data_freq)\n",
    "\n",
    "    # Get time of day from the associated datetime column\n",
    "    time_of_day = pd.Series(power_time_series.index.hour +\n",
    "                            power_time_series.index.minute/60,\n",
    "                            index=power_time_series.index)\n",
    "    # Pivot the dataframe\n",
    "    dataframe = pd.DataFrame(pd.concat([power_time_series, time_of_day], axis=1))\n",
    "    dataframe.columns = [\"values\", 'time_of_day']\n",
    "    dataframe = dataframe.dropna()\n",
    "    dataframe_pivoted = dataframe.pivot_table(index='time_of_day',\n",
    "                                              columns=dataframe.index.date,\n",
    "                                              values=\"values\")\n",
    "\n",
    "    # Get the modeled sunrise and sunset time series based on the system's\n",
    "    # latitude-longitude coordinates\n",
    "    modeled_sunrise_sunset_df = pvlib.solarposition.sun_rise_set_transit_spa(\n",
    "         power_time_series.index, latitude, longitude)\n",
    "\n",
    "    # Calculate the midday point between sunrise and sunset for each day\n",
    "    # in the modeled irradiance series\n",
    "    modeled_midday_series = modeled_sunrise_sunset_df['sunrise'] + \\\n",
    "        (modeled_sunrise_sunset_df['sunset'] -\n",
    "         modeled_sunrise_sunset_df['sunrise']) / 2\n",
    "\n",
    "    # Run day-night mask on the power time series\n",
    "    daytime_mask = power_or_irradiance(power_time_series,\n",
    "                                       freq=data_freq,\n",
    "                                       low_value_threshold=.005)\n",
    "\n",
    "    # Generate the sunrise, sunset, and halfway points for the data stream\n",
    "    sunrise_series = daytime.get_sunrise(daytime_mask)\n",
    "    sunset_series = daytime.get_sunset(daytime_mask)\n",
    "    midday_series = sunrise_series + ((sunset_series - sunrise_series)/2)\n",
    "\n",
    "    # Convert the midday and modeled midday series to daily values\n",
    "    midday_series_daily, modeled_midday_series_daily = (\n",
    "        midday_series.resample('D').mean(),\n",
    "        modeled_midday_series.resample('D').mean())\n",
    "\n",
    "    # Set midday value series as minutes since midnight, from midday datetime\n",
    "    # values\n",
    "    midday_series_daily = (midday_series_daily.dt.hour * 60 +\n",
    "                           midday_series_daily.dt.minute +\n",
    "                           midday_series_daily.dt.second / 60)\n",
    "    modeled_midday_series_daily = \\\n",
    "        (modeled_midday_series_daily.dt.hour * 60 +\n",
    "         modeled_midday_series_daily.dt.minute +\n",
    "         modeled_midday_series_daily.dt.second / 60)\n",
    "\n",
    "    # Estimate the time shifts by comparing the modelled midday point to the\n",
    "    # measured midday point.\n",
    "    is_shifted, time_shift_series = shifts_ruptures(midday_series_daily,\n",
    "                                                    modeled_midday_series_daily,\n",
    "                                                    period_min=15,\n",
    "                                                    shift_min=15,\n",
    "                                                    zscore_cutoff=1.5)\n",
    "\n",
    "    # Create a midday difference series between modeled and measured midday, to\n",
    "    # visualize time shifts. First, resample each time series to daily frequency,\n",
    "    # and compare the data stream's daily halfway point to the modeled halfway\n",
    "    # point\n",
    "    midday_diff_series = (midday_series.resample('D').mean() -\n",
    "                          modeled_midday_series.resample('D').mean()\n",
    "                          ).dt.total_seconds() / 60\n",
    "\n",
    "    # Generate boolean for detected time shifts\n",
    "    if any(time_shift_series != 0):\n",
    "        time_shifts_detected = True\n",
    "    else:\n",
    "        time_shifts_detected = False\n",
    "\n",
    "    # Build a list of time shifts for re-indexing. We choose to use dicts.\n",
    "    time_shift_series.index = pd.to_datetime(\n",
    "        time_shift_series.index)\n",
    "    changepoints = (time_shift_series != time_shift_series.shift(1))\n",
    "    changepoints = changepoints[changepoints].index\n",
    "    changepoint_amts = pd.Series(time_shift_series.loc[changepoints])\n",
    "    time_shift_list = list()\n",
    "    for idx in range(len(changepoint_amts)):\n",
    "        if changepoint_amts[idx] == 0:\n",
    "            change_amt = 0\n",
    "        else:\n",
    "            change_amt = -1 * changepoint_amts[idx]\n",
    "        if idx < (len(changepoint_amts) - 1):\n",
    "            time_shift_list.append({\"datetime_start\":\n",
    "                                    str(changepoint_amts.index[idx]),\n",
    "                                    \"datetime_end\":\n",
    "                                        str(changepoint_amts.index[idx + 1]),\n",
    "                                    \"time_shift\": change_amt})\n",
    "        else:\n",
    "            time_shift_list.append({\"datetime_start\":\n",
    "                                    str(changepoint_amts.index[idx]),\n",
    "                                    \"datetime_end\":\n",
    "                                        str(time_shift_series.index.max()),\n",
    "                                    \"time_shift\": change_amt})\n",
    "\n",
    "    # Correct any time shifts in the time series\n",
    "    new_index = pd.Series(power_time_series.index, index=power_time_series.index).dropna()\n",
    "    for i in time_shift_list:\n",
    "        if pd.notna(i['time_shift']):\n",
    "            new_index[(power_time_series.index >= pd.to_datetime(i['datetime_start'])) &\n",
    "                  (power_time_series.index < pd.to_datetime(i['datetime_end']))] = \\\n",
    "            power_time_series.index + pd.Timedelta(minutes=i['time_shift'])\n",
    "    power_time_series.index = new_index\n",
    "\n",
    "    # Remove duplicated indices and sort the time series (just in case)\n",
    "    power_time_series = power_time_series[~power_time_series.index.duplicated(\n",
    "        keep='first')].sort_index()\n",
    "\n",
    "    # Set all values in the nighttime mask to 0\n",
    "    power_time_series.loc[~daytime_mask] = 0\n",
    "    # Resample the time series to daily mean\n",
    "    power_time_series_daily = power_time_series.resample('D').mean()\n",
    "    data_shift_start_date, data_shift_end_date = \\\n",
    "        ds.get_longest_shift_segment_dates(power_time_series_daily,\n",
    "                                           use_default_models=False,\n",
    "                                           method=rpt.Binseg, cost='rbf',\n",
    "                                           penalty=15)\n",
    "    data_shift_period_length = (data_shift_end_date -\n",
    "                                data_shift_start_date).days\n",
    "\n",
    "    # Get the number of shift dates\n",
    "    data_shift_mask = ds.detect_data_shifts(power_time_series_daily,\n",
    "                                            use_default_models=False,\n",
    "                                            method=rpt.Binseg, cost='rbf',\n",
    "                                            penalty=15)\n",
    "    # Get the shift dates\n",
    "    shift_dates = list(power_time_series_daily[data_shift_mask].index)\n",
    "    if len(shift_dates) > 0:\n",
    "        shift_found = True\n",
    "    else:\n",
    "        shift_found = False\n",
    "    \n",
    "    power_time_series = power_time_series[\n",
    "        (power_time_series.index >=\n",
    "         data_shift_start_date.tz_convert(power_time_series.index.tz)) &\n",
    "        (power_time_series.index <=\n",
    "         data_shift_end_date.tz_convert(power_time_series.index.tz))]\n",
    "\n",
    "    power_time_series = power_time_series.asfreq(data_freq)\n",
    "    power_time_series.to_csv(os.path.join(\"./data\",  col + \".pkl\"))\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
