{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc3b6a3c",
   "metadata": {},
   "source": [
    "# PVAnalytics QA Process: Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a262667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pvanalytics\n",
    "import numpy as np\n",
    "import rdtools\n",
    "from statistics import mode\n",
    "import json\n",
    "# pvanalytics.__version__\n",
    "from pvanalytics.features.clearsky import reno       #update to just do a pvanalytics import?\n",
    "import pvlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pvanalytics.quality import data_shifts as ds\n",
    "from pvanalytics.quality import gaps\n",
    "from pvanalytics.quality.outliers import zscore\n",
    "from pvanalytics.features.daytime import power_or_irradiance\n",
    "from pvanalytics.quality.time import shifts_ruptures\n",
    "from pvanalytics.features import daytime\n",
    "from pvanalytics.system import (is_tracking_envelope,\n",
    "                                infer_orientation_fit_pvwatts)\n",
    "from pvanalytics.features.clipping import geometric\n",
    "import ruptures as rpt\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 12,\n",
    "                           'figure.figsize': [4.5, 3],\n",
    "                           'lines.markeredgewidth': 0,\n",
    "                           'lines.markersize': 2\n",
    "                           })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12cebbf",
   "metadata": {},
   "source": [
    "In the following example, a process for assessing the data quality of the AC power data streams for system 2107 is shown, using PVAnalytics functions. This example pipeline illustrates how several PVAnalytics functions can be used in sequence to assess the quality of a power data stream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae06055",
   "metadata": {},
   "source": [
    "First, we download and import the power data from a PV installation under the [2023 solar data prize data set](https://data.openei.org/s3_viewer?bucket=oedi-data-lake&limit=100&prefix=pvdaq%2F2023-solar-data-prize%2F). This data set is publicly available via the PVDAQ database in the DOE Open Energy Data Initiative (OEDI) (https://data.openei.org/submissions/4568), under system ID 2107. This data is timezone-localized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fba4db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local CSV file: ./data/2107_electrical_data.csv\n"
     ]
    }
   ],
   "source": [
    "with open('./data/2107_system_metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "tz = metadata['System']['timezone_code']\n",
    "\n",
    "def load_data(filename, s3_bucket, s3_key):\n",
    "    local_file_path = filename\n",
    "    # Check if the file exists locally\n",
    "    if os.path.exists(local_file_path):\n",
    "        print(f\"Loading local CSV file: {local_file_path}\")\n",
    "    else:\n",
    "        print(f\"Local CSV file not found. Downloading from S3.\")\n",
    "        download_csv_from_s3(s3_bucket, s3_key, local_file_path)\n",
    "    data_frame = load_csv(local_file_path)\n",
    "    return data_frame\n",
    " \n",
    "def download_csv_from_s3(bucket_name, s3_key, local_destination):\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    s3.download_file(bucket_name, s3_key, local_destination)\n",
    " \n",
    "def load_csv(file_path):\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        index_col=0,\n",
    "        parse_dates=[0],\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df_elect = load_data(\n",
    "    filename=\"./data/2107_electrical_data.csv\",\n",
    "    s3_bucket=\"oedi-data-lake\",\n",
    "    s3_key=\"pvdaq/2023-solar-data-prize/2107_OEDI/data/2107_electrical_data.csv\")\n",
    "\n",
    "\n",
    "df_elect = df_elect.tz_localize(tz, ambiguous=True)\n",
    "\n",
    "power_columns = [x for x in df_elect.columns if 'power' in x]\n",
    "\n",
    "latitude = metadata['Site']['latitude']\n",
    "longitude = metadata['Site']['longitude']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e942d6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'seconds'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m data_freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5min\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# REMOVE STALE DATA (that isn't during nighttime periods)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Day/night mask\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m daytime_mask \u001b[38;5;241m=\u001b[39m \u001b[43mpower_or_irradiance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpower_time_series\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Stale data mask\u001b[39;00m\n\u001b[0;32m     11\u001b[0m stale_data_mask \u001b[38;5;241m=\u001b[39m gaps\u001b[38;5;241m.\u001b[39mstale_values_round(power_time_series,\n\u001b[0;32m     12\u001b[0m                                           window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m     13\u001b[0m                                           decimals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pvfleets_qa_analysis\\lib\\site-packages\\pvanalytics\\features\\daytime.py:197\u001b[0m, in \u001b[0;36mpower_or_irradiance\u001b[1;34m(series, outliers, low_value_threshold, low_median_threshold, low_diff_threshold, median_days, clipping, freq, correction_window, hours_min, day_length_difference_max, day_length_window)\u001b[0m\n\u001b[0;32m    195\u001b[0m series \u001b[38;5;241m=\u001b[39m series\u001b[38;5;241m.\u001b[39mfillna(value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    196\u001b[0m series_norm \u001b[38;5;241m=\u001b[39m _filter_and_normalize(series, outliers)\u001b[38;5;241m.\u001b[39mfillna(value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 197\u001b[0m minutes_per_value \u001b[38;5;241m=\u001b[39m \u001b[43m_freqstr_to_minutes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_freq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m first_order_diff \u001b[38;5;241m=\u001b[39m series_norm\u001b[38;5;241m.\u001b[39mdiff() \u001b[38;5;241m/\u001b[39m minutes_per_value\n\u001b[0;32m    201\u001b[0m rolling_median \u001b[38;5;241m=\u001b[39m _rolling_by_minute(\n\u001b[0;32m    202\u001b[0m     series_norm,\n\u001b[0;32m    203\u001b[0m     days\u001b[38;5;241m=\u001b[39mmedian_days,\n\u001b[0;32m    204\u001b[0m     f\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mcore\u001b[38;5;241m.\u001b[39mwindow\u001b[38;5;241m.\u001b[39mRollingGroupby\u001b[38;5;241m.\u001b[39mmedian\n\u001b[0;32m    205\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pvfleets_qa_analysis\\lib\\site-packages\\pvanalytics\\features\\daytime.py:107\u001b[0m, in \u001b[0;36m_freqstr_to_minutes\u001b[1;34m(freqstr)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_freqstr_to_minutes\u001b[39m(freqstr):\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfreq_to_timedelta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfreqstr\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseconds\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'seconds'"
     ]
    }
   ],
   "source": [
    "for col in power_columns:\n",
    "    power_time_series = df_elect[col].copy()\n",
    "\n",
    "    # Get the time frequency of the time series\n",
    "    data_freq = \"5min\"\n",
    "\n",
    "    # REMOVE STALE DATA (that isn't during nighttime periods)\n",
    "    # Day/night mask\n",
    "    daytime_mask = power_or_irradiance(power_time_series)\n",
    "    # Stale data mask\n",
    "    stale_data_mask = gaps.stale_values_round(power_time_series,\n",
    "                                              window=3,\n",
    "                                              decimals=2)\n",
    "    stale_data_mask = stale_data_mask & daytime_mask\n",
    "\n",
    "    # REMOVE NEGATIVE DATA\n",
    "    negative_mask = (power_time_series < 0)\n",
    "\n",
    "    # FIND ABNORMAL PERIODS\n",
    "    daily_min = power_time_series.resample('D').min()\n",
    "    series_min = 0.1 * power_time_series.mean()\n",
    "    erroneous_mask = (daily_min >= series_min)\n",
    "    erroneous_mask = erroneous_mask.reindex(index=power_time_series.index,\n",
    "                                            method='ffill',\n",
    "                                            fill_value=False)\n",
    "\n",
    "    # FIND OUTLIERS (Z-SCORE FILTER)\n",
    "    zscore_outlier_mask = zscore(power_time_series, zmax=4,\n",
    "                                 nan_policy='omit')\n",
    "\n",
    "    # Get the percentage of data flagged for each issue, so it can later be logged\n",
    "    pct_stale = round((len(power_time_series[\n",
    "        stale_data_mask].dropna())/len(power_time_series.dropna())*100), 1)\n",
    "    pct_negative = round((len(power_time_series[\n",
    "        negative_mask].dropna())/len(power_time_series.dropna())*100), 1)\n",
    "    pct_erroneous = round((len(power_time_series[\n",
    "        erroneous_mask].dropna())/len(power_time_series.dropna())*100), 1)\n",
    "    pct_outlier = round((len(power_time_series[\n",
    "        zscore_outlier_mask].dropna())/len(power_time_series.dropna())*100), 1)\n",
    "\n",
    "\n",
    "\n",
    "    # Filter the time series, taking out all of the issues\n",
    "    issue_mask = ((~stale_data_mask) & (~negative_mask) &\n",
    "              (~erroneous_mask) & (~zscore_outlier_mask))\n",
    "\n",
    "    power_time_series = power_time_series[issue_mask].copy()\n",
    "    power_time_series = power_time_series.asfreq(data_freq)\n",
    "\n",
    "\n",
    "    # daily data completeness\n",
    "    x = power_time_series.copy()\n",
    "    x.loc[~daytime_mask] = 0\n",
    "    data_completeness_score = gaps.completeness_score(x)\n",
    "\n",
    "\n",
    "    # Trim the series based on daily completeness score\n",
    "    trim_series = pvanalytics.quality.gaps.trim_incomplete(\n",
    "        x, minimum_completeness=.25, freq=data_freq)\n",
    "\n",
    "    power_time_series = power_time_series[trim_series].copy()\n",
    "    power_time_series = power_time_series.asfreq(data_freq)\n",
    "\n",
    "    # Get time of day from the associated datetime column\n",
    "    time_of_day = pd.Series(power_time_series.index.hour +\n",
    "                            power_time_series.index.minute/60,\n",
    "                            index=power_time_series.index)\n",
    "    # Pivot the dataframe\n",
    "    dataframe = pd.DataFrame(pd.concat([power_time_series, time_of_day], axis=1))\n",
    "    dataframe.columns = [\"values\", 'time_of_day']\n",
    "    dataframe = dataframe.dropna()\n",
    "    dataframe_pivoted = dataframe.pivot_table(index='time_of_day',\n",
    "                                              columns=dataframe.index.date,\n",
    "                                              values=\"values\")\n",
    "\n",
    "    # Get the modeled sunrise and sunset time series based on the system's\n",
    "    # latitude-longitude coordinates\n",
    "    modeled_sunrise_sunset_df = pvlib.solarposition.sun_rise_set_transit_spa(\n",
    "         power_time_series.index, latitude, longitude)\n",
    "\n",
    "    # Calculate the midday point between sunrise and sunset for each day\n",
    "    # in the modeled irradiance series\n",
    "    modeled_midday_series = modeled_sunrise_sunset_df['sunrise'] + \\\n",
    "        (modeled_sunrise_sunset_df['sunset'] -\n",
    "         modeled_sunrise_sunset_df['sunrise']) / 2\n",
    "\n",
    "    # Run day-night mask on the power time series\n",
    "    daytime_mask = power_or_irradiance(power_time_series,\n",
    "                                       freq=data_freq,\n",
    "                                       low_value_threshold=.005)\n",
    "\n",
    "    # Generate the sunrise, sunset, and halfway points for the data stream\n",
    "    sunrise_series = daytime.get_sunrise(daytime_mask)\n",
    "    sunset_series = daytime.get_sunset(daytime_mask)\n",
    "    midday_series = sunrise_series + ((sunset_series - sunrise_series)/2)\n",
    "\n",
    "    # Convert the midday and modeled midday series to daily values\n",
    "    midday_series_daily, modeled_midday_series_daily = (\n",
    "        midday_series.resample('D').mean(),\n",
    "        modeled_midday_series.resample('D').mean())\n",
    "\n",
    "    # Set midday value series as minutes since midnight, from midday datetime\n",
    "    # values\n",
    "    midday_series_daily = (midday_series_daily.dt.hour * 60 +\n",
    "                           midday_series_daily.dt.minute +\n",
    "                           midday_series_daily.dt.second / 60)\n",
    "    modeled_midday_series_daily = \\\n",
    "        (modeled_midday_series_daily.dt.hour * 60 +\n",
    "         modeled_midday_series_daily.dt.minute +\n",
    "         modeled_midday_series_daily.dt.second / 60)\n",
    "\n",
    "    # Estimate the time shifts by comparing the modelled midday point to the\n",
    "    # measured midday point.\n",
    "    is_shifted, time_shift_series = shifts_ruptures(midday_series_daily,\n",
    "                                                    modeled_midday_series_daily,\n",
    "                                                    period_min=15,\n",
    "                                                    shift_min=15,\n",
    "                                                    zscore_cutoff=1.5)\n",
    "\n",
    "    # Create a midday difference series between modeled and measured midday, to\n",
    "    # visualize time shifts. First, resample each time series to daily frequency,\n",
    "    # and compare the data stream's daily halfway point to the modeled halfway\n",
    "    # point\n",
    "    midday_diff_series = (midday_series.resample('D').mean() -\n",
    "                          modeled_midday_series.resample('D').mean()\n",
    "                          ).dt.total_seconds() / 60\n",
    "\n",
    "    # Generate boolean for detected time shifts\n",
    "    if any(time_shift_series != 0):\n",
    "        time_shifts_detected = True\n",
    "    else:\n",
    "        time_shifts_detected = False\n",
    "\n",
    "    # Build a list of time shifts for re-indexing. We choose to use dicts.\n",
    "    time_shift_series.index = pd.to_datetime(\n",
    "        time_shift_series.index)\n",
    "    changepoints = (time_shift_series != time_shift_series.shift(1))\n",
    "    changepoints = changepoints[changepoints].index\n",
    "    changepoint_amts = pd.Series(time_shift_series.loc[changepoints])\n",
    "    time_shift_list = list()\n",
    "    for idx in range(len(changepoint_amts)):\n",
    "        if changepoint_amts[idx] == 0:\n",
    "            change_amt = 0\n",
    "        else:\n",
    "            change_amt = -1 * changepoint_amts[idx]\n",
    "        if idx < (len(changepoint_amts) - 1):\n",
    "            time_shift_list.append({\"datetime_start\":\n",
    "                                    str(changepoint_amts.index[idx]),\n",
    "                                    \"datetime_end\":\n",
    "                                        str(changepoint_amts.index[idx + 1]),\n",
    "                                    \"time_shift\": change_amt})\n",
    "        else:\n",
    "            time_shift_list.append({\"datetime_start\":\n",
    "                                    str(changepoint_amts.index[idx]),\n",
    "                                    \"datetime_end\":\n",
    "                                        str(time_shift_series.index.max()),\n",
    "                                    \"time_shift\": change_amt})\n",
    "\n",
    "    # Correct any time shifts in the time series\n",
    "    new_index = pd.Series(power_time_series.index, index=power_time_series.index).dropna()\n",
    "    for i in time_shift_list:\n",
    "        if pd.notna(i['time_shift']):\n",
    "            new_index[(power_time_series.index >= pd.to_datetime(i['datetime_start'])) &\n",
    "                  (power_time_series.index < pd.to_datetime(i['datetime_end']))] = \\\n",
    "            power_time_series.index + pd.Timedelta(minutes=i['time_shift'])\n",
    "    power_time_series.index = new_index\n",
    "\n",
    "    # Remove duplicated indices and sort the time series (just in case)\n",
    "    power_time_series = power_time_series[~power_time_series.index.duplicated(\n",
    "        keep='first')].sort_index()\n",
    "\n",
    "    # Set all values in the nighttime mask to 0\n",
    "    power_time_series.loc[~daytime_mask] = 0\n",
    "    # Resample the time series to daily mean\n",
    "    power_time_series_daily = power_time_series.resample('D').mean()\n",
    "    data_shift_start_date, data_shift_end_date = \\\n",
    "        ds.get_longest_shift_segment_dates(power_time_series_daily,\n",
    "                                           use_default_models=False,\n",
    "                                           method=rpt.Binseg, cost='rbf',\n",
    "                                           penalty=15)\n",
    "    data_shift_period_length = (data_shift_end_date -\n",
    "                                data_shift_start_date).days\n",
    "\n",
    "    # Get the number of shift dates\n",
    "    data_shift_mask = ds.detect_data_shifts(power_time_series_daily,\n",
    "                                            use_default_models=False,\n",
    "                                            method=rpt.Binseg, cost='rbf',\n",
    "                                            penalty=15)\n",
    "    # Get the shift dates\n",
    "    shift_dates = list(power_time_series_daily[data_shift_mask].index)\n",
    "    if len(shift_dates) > 0:\n",
    "        shift_found = True\n",
    "    else:\n",
    "        shift_found = False\n",
    "    \n",
    "    power_time_series = power_time_series[\n",
    "        (power_time_series.index >=\n",
    "         data_shift_start_date.tz_convert(power_time_series.index.tz)) &\n",
    "        (power_time_series.index <=\n",
    "         data_shift_end_date.tz_convert(power_time_series.index.tz))]\n",
    "\n",
    "    power_time_series = power_time_series.asfreq(data_freq)\n",
    "    power_time_series.to_csv(os.path.join(\"./data\",  col + \".pkl\"))\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
